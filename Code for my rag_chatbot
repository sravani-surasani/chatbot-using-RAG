UPDATED CODE: I updated the code which i shown in video that code consist of more lines so i minimize the code into lesser lines with same output. Can you please consider these code.

# Multi-Agent RAG System with Model Context Protocol (MCP)

import os, uuid, json, asyncio, logging, tempfile, shutil
from datetime import datetime
from typing import Dict, List, Any
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
import streamlit as st, pandas as pd
from sentence_transformers import SentenceTransformer
import faiss, PyPDF2, docx
from pptx import Presentation
from openai import OpenAI

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class MCPMessage:
    sender: str; receiver: str; type: str; trace_id: str; payload: Dict[str, Any]; timestamp: str = None
    def __post_init__(self): self.timestamp = self.timestamp or datetime.now().isoformat()
    def to_dict(self): return asdict(self)
    @classmethod
    def from_dict(cls, data): return cls(**data)

class MessageType:
    DOCUMENT_UPLOAD = "DOCUMENT_UPLOAD"
    INGESTION_REQUEST = "INGESTION_REQUEST"
    INGESTION_RESPONSE = "INGESTION_RESPONSE"
    RETRIEVAL_REQUEST = "RETRIEVAL_REQUEST"
    RETRIEVAL_RESPONSE = "RETRIEVAL_RESPONSE"
    LLM_REQUEST = "LLM_REQUEST"
    LLM_RESPONSE = "LLM_RESPONSE"
    ERROR = "ERROR"

class MessageBus:
    def __init__(self): self.subscribers = {}; self.message_history = []
    def subscribe(self, agent_id, callback):
        self.subscribers.setdefault(agent_id, []).append(callback)
    async def publish(self, message):
        self.message_history.append(message)
        logger.info(f"Publishing message: {message.sender} -> {message.receiver} ({message.type})")
        for callback in self.subscribers.get(message.receiver, []):
            try: await callback(message)
            except Exception as e: logger.error(f"Error delivering message to {message.receiver}: {e}")
    def get_message_history(self, trace_id=None):
        return [m for m in self.message_history if m.trace_id == trace_id] if trace_id else self.message_history

class BaseAgent(ABC):
    def __init__(self, agent_id, bus):
        self.agent_id, self.message_bus = agent_id, bus
        self.message_bus.subscribe(agent_id, self.handle_message)
    @abstractmethod
    async def handle_message(self, message): pass
    async def send_message(self, receiver, msg_type, payload, trace_id):
        await self.message_bus.publish(MCPMessage(self.agent_id, receiver, msg_type, trace_id, payload))

class DocumentParser:
    @staticmethod
    def parse_pdf(fp):
        try: return "\n".join(page.extract_text() for page in PyPDF2.PdfReader(fp).pages)
        except Exception as e: logger.error(f"PDF error {fp}: {e}"); return ""
    @staticmethod
    def parse_docx(fp):
        try: return "\n".join(p.text for p in docx.Document(fp).paragraphs)
        except Exception as e: logger.error(f"DOCX error {fp}: {e}"); return ""
    @staticmethod
    def parse_pptx(fp):
        try: return "\n".join(shape.text for s in Presentation(fp).slides for shape in s.shapes if hasattr(shape, "text"))
        except Exception as e: logger.error(f"PPTX error {fp}: {e}"); return ""
    @staticmethod
    def parse_csv(fp):
        try: return pd.read_csv(fp).to_string()
        except Exception as e: logger.error(f"CSV error {fp}: {e}"); return ""
    @staticmethod
    def parse_txt(fp):
        try: return open(fp, encoding='utf-8').read()
        except Exception as e: logger.error(f"TXT error {fp}: {e}"); return ""

class IngestionAgent(BaseAgent):
    def __init__(self, bus): super().__init__("IngestionAgent", bus); self.parser = DocumentParser(); self.docs = {}
    async def handle_message(self, msg):
        if msg.type == MessageType.INGESTION_REQUEST:
            try:
                docs, results = msg.payload.get("documents", []), []
                for d in docs:
                    fp, fn, ft = d["file_path"], d["file_name"], d["file_type"]
                    text = getattr(self.parser, f"parse_{ft}", lambda x: "")(fp)
                    chunks = self.chunk_text(text)
                    doc = {"file_name": fn, "file_type": ft, "text": text, "chunks": chunks, "processed_at": datetime.now().isoformat()}
                    results.append(doc); self.docs[fn] = doc
                await self.send_message("RetrievalAgent", MessageType.INGESTION_RESPONSE, {"processed_documents": results}, msg.trace_id)
            except Exception as e:
                await self.send_message("CoordinatorAgent", MessageType.ERROR, {"error": str(e)}, msg.trace_id)
    def chunk_text(self, t, size=500, overlap=50):
        return [" ".join(t.split()[i:i + size]) for i in range(0, len(t.split()), size - overlap)]

class RetrievalAgent(BaseAgent):
    def __init__(self, bus): super().__init__("RetrievalAgent", bus); self.model = SentenceTransformer('all-MiniLM-L6-v2'); self.chunks = []; self.index = None
    async def handle_message(self, msg):
        if msg.type == MessageType.INGESTION_RESPONSE: await self.build_index(msg)
        elif msg.type == MessageType.RETRIEVAL_REQUEST: await self.retrieve(msg)
    async def build_index(self, msg):
        try:
            self.chunks = [{"text": c, "source": d["file_name"], "file_type": d["file_type"]} for d in msg.payload["processed_documents"] for c in d["chunks"]]
            if self.chunks:
                emb = self.model.encode([c["text"] for c in self.chunks]).astype('float32')
                self.index = faiss.IndexFlatL2(emb.shape[1]); self.index.add(emb)
        except Exception as e: logger.error(f"Vector build error: {e}")
    async def retrieve(self, msg):
        try:
            q = msg.payload.get("query", ""); k = msg.payload.get("top_k", 5)
            if not self.index or not q: return await self.send_message("LLMResponseAgent", MessageType.RETRIEVAL_RESPONSE, {"retrieved_context": [], "query": q}, msg.trace_id)
            d, i = self.index.search(self.model.encode([q]).astype('float32'), k)
            ctx = [{**self.chunks[x], "score": float(d[0][j])} for j, x in enumerate(i[0]) if x < len(self.chunks)]
            await self.send_message("LLMResponseAgent", MessageType.RETRIEVAL_RESPONSE, {"retrieved_context": ctx, "query": q}, msg.trace_id)
        except Exception as e:
            await self.send_message("CoordinatorAgent", MessageType.ERROR, {"error": str(e)}, msg.trace_id)

class LLMResponseAgent(BaseAgent):
    def __init__(self, bus, api_key=None):
        super().__init__("LLMResponseAgent", bus); self.client = OpenAI(api_key=api_key) if api_key else None
    async def handle_message(self, msg):
        if msg.type == MessageType.RETRIEVAL_RESPONSE: await self.generate(msg)
    async def generate(self, msg):
        try:
            q, ctx = msg.payload.get("query", ""), msg.payload.get("retrieved_context", [])
            text = "\n\n".join([f"[Source {i+1}: {c['source']}]:\n{c['text']}" for i, c in enumerate(ctx)])
            sources = [{"source": c["source"], "file_type": c["file_type"], "text": c["text"][:200] + "..."} for c in ctx]
            resp = await self.call_openai(q, text) if self.client else self.fallback(q, ctx)
            await self.send_message("CoordinatorAgent", MessageType.LLM_RESPONSE, {"query": q, "response": resp, "sources": sources, "context_used": len(ctx)}, msg.trace_id)
        except Exception as e:
            await self.send_message("CoordinatorAgent", MessageType.ERROR, {"error": str(e)}, msg.trace_id)
    async def call_openai(self, q, ctx):
        try:
            r = self.client.chat.completions.create(model="gpt-3.5-turbo", messages=[{"role": "system", "content": "You are helpful."}, {"role": "user", "content": f"Context:\n{ctx}\n\nQuestion: {q}"}], max_tokens=500)
            return r.choices[0].message.content
        except Exception as e: return f"OpenAI error: {e}"
    def fallback(self, q, ctx):
        if not ctx: return "No relevant info found."
        return f"Found {len(ctx)} results for '{q}':\n\n" + "\n\n".join([f"From {c['source']}:\n{c['text'][:300]}..." for c in ctx[:3]])

class CoordinatorAgent(BaseAgent):
    def __init__(self, bus): super().__init__("CoordinatorAgent", bus); self.traces = {}
    async def handle_message(self, msg):
        if msg.type == MessageType.LLM_RESPONSE: self.complete(msg)
        elif msg.type == MessageType.ERROR: self.fail(msg)
    def complete(self, msg):
        t = self.traces.get(msg.trace_id, {}); t.update({"status": "done", "response": msg.payload.get("response"), "sources": msg.payload.get("sources", [])})
    def fail(self, msg): self.traces[msg.trace_id] = {"status": "error", "error": msg.payload.get("error")}
    async def process_user_query(self, q, files=None):
        trace_id = str(uuid.uuid4()); self.traces[trace_id] = {"query": q, "status": "processing"}
        try:
            if files:
                await self.send_message("IngestionAgent", MessageType.INGESTION_REQUEST, {"documents": files}, trace_id)
                await asyncio.sleep(2)
            await self.send_message("RetrievalAgent", MessageType.RETRIEVAL_REQUEST, {"query": q, "top_k": 5}, trace_id)
            for _ in range(60):
                await asyncio.sleep(0.5)
                if self.traces[trace_id]["status"] != "processing": break
            return self.traces[trace_id].get("response", "Timeout: No response")
        except Exception as e: return f"Coordinator error: {e}"

class MultiAgentRAGSystem:
    def __init__(self, key=None):
        self.bus = MessageBus(); self.ing = IngestionAgent(self.bus)
        self.ret = RetrievalAgent(self.bus)
        self.llm = LLMResponseAgent(self.bus, key)
        self.coord = CoordinatorAgent(self.bus)
    async def process_query(self, q, files=None):
        res = await self.coord.process_user_query(q, files)
        for tid, data in self.coord.traces.items():
            if data.get("query") == q: return {"response": res, "trace_info": data, "sources": data.get("sources", [])}
        return {"response": res, "trace_info": None, "sources": []}

def save_uploaded_file(f):
    d = tempfile.mkdtemp(); p = os.path.join(d, f.name)
    with open(p, "wb") as o: o.write(f.getbuffer())
    return p, d

def get_file_type(n): return n.lower().split('.')[-1]

def main():
    st.set_page_config(page_title="Multi-Agent RAG", page_icon="ðŸ¤–", layout="wide")
    st.title("ðŸ¤– Multi-Agent RAG System with MCP")
    if 'rag_system' not in st.session_state:
        k = st.sidebar.text_input("OpenAI API Key", type="password", value=os.getenv("OPENAI_API_KEY", ""))
        st.session_state.rag_system = MultiAgentRAGSystem(k)
    if 'messages' not in st.session_state: st.session_state.messages = []

    with st.sidebar:
        st.header("ðŸ“ Document Upload")
        uploaded = st.file_uploader("Upload documents", type=['pdf', 'docx', 'pptx', 'csv', 'txt', 'md'], accept_multiple_files=True)
        if uploaded: st.success(f"âœ… {len(uploaded)} files uploaded")

    st.header("ðŸ’¬ Chat Interface")
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])
            if m["role"] == "assistant" and "sources" in m and m["sources"]:
                st.markdown("**Sources:**")
                for s in m["sources"]:
                    st.markdown(f"- {s['source']} ({s['file_type']})")

    if prompt := st.chat_input("Ask a question..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"): st.markdown(prompt)
        uploaded_info, cleanup_paths = [], []
        if uploaded:
            for f in uploaded:
                path, temp_dir = save_uploaded_file(f)
                uploaded_info.append({"file_name": f.name, "file_path": path, "file_type": get_file_type(f.name)})
                cleanup_paths.append(temp_dir)
        with st.chat_message("assistant"):
            with st.spinner("Processing..."):
                try:
                    r = asyncio.run(st.session_state.rag_system.process_query(prompt, uploaded_info))
                    st.markdown(r["response"])
                    if r["sources"]:
                        st.markdown("**Sources:**")
                        for s in r["sources"]:
                            st.markdown(f"- {s['source']} ({s['file_type']})")
                    st.session_state.messages.append({"role": "assistant", "content": r["response"], "sources": r["sources"]})
                except Exception as e:
                    st.error(f"Error: {e}")
                    st.session_state.messages.append({"role": "assistant", "content": str(e)})
        for d in cleanup_paths:
            try:
                for root, dirs, files in os.walk(d, topdown=False):
                    for name in files:
                        try: os.remove(os.path.join(root, name))
                        except: pass
                    for name in dirs:
                        try: os.rmdir(os.path.join(root, name))
                        except: pass
                os.rmdir(d)
            except Exception as e: logger.warning(f"Cleanup failed: {e}")

    if st.sidebar.checkbox("Show Debug Info"):
        st.sidebar.header("ðŸ” Debug Info")
        h = st.session_state.rag_system.bus.get_message_history()
        if h:
            st.sidebar.write(f"Total Messages: {len(h)}")
            for msg in h[-2:]:
                st.sidebar.json({"sender": msg.sender, "receiver": msg.receiver, "type": msg.type, "timestamp": msg.timestamp})

if __name__ == "__main__": main()
